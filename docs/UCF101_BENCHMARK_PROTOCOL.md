# UCF101 Benchmark Protocol (Evidence-Grade)

## What is considered proof?
We provide **artifact-level proof**:
- `preds.npz` stores `y_true` and `y_pred` arrays for the official UCF101 test list.
- `ids.txt` lists the exact sample IDs aligned with those arrays.
- `metrics.json` stores computed metrics.
- `scripts/verify_benchmark_results.py` recomputes metrics from `preds.npz` and verifies they match both
  `metrics.json` and the README benchmark table.

This means the benchmark table is not a claim: it is verified by recomputation from committed artifacts.

## Dataset
- UCF101 (101 classes)
- Official split lists:
  - `trainlist01.txt`, `testlist01.txt` for Split-1 (and optionally 02/03)
  - `classInd.txt` for class-to-index mapping

Files must be placed under:
- `data/ucf101/splits/`

## Feature representation
This benchmark assumes skeleton/pose (or motion) features are extracted from UCF101 videos into `.npz` files.

Expected format per sample:
- Location: `data/ucf101/processed/<ClassName>/<VideoName>.npz`
- Keys:
  - `x`: float32 array of shape [T, V, C] (frames, joints, channels)
- Recommended:
  - C=2 for (x,y) or C=3 for (x,y,confidence)
  - Fixed clip length handled by center crop/pad inside the dataloader

## Manifest
To freeze the exact processed dataset used, we store:
- `data/ucf101/manifest_sha256.txt`

Generated by:
`python scripts/make_manifest_ucf101.py`

The manifest is a cryptographic fingerprint of the processed features used.

## Differential Privacy
For DP runs:
- DP-SGD implemented using Opacus
- Configuration fixed by the benchmark config:
  - `noise_multiplier`
  - `max_grad_norm`
  - `delta`
- We also record:
  - achieved epsilon computed from the Opacus accountant

DP parameters are saved into `dp.json` for each run.

## Inference benchmark
We measure:
- mean latency (ms)
- p95 latency (ms)
- throughput (FPS)
- model size (MB)

Results are saved into `infer.json`.
